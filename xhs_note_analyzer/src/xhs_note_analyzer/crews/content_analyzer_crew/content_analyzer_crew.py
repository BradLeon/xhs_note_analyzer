#!/usr/bin/env python

"""
Content Analyzer Crew
å°çº¢ä¹¦å†…å®¹å¤šç»´åº¦åˆ†æCrew - Step3å®ç°

è´Ÿè´£å¯¹å°çº¢ä¹¦ç¬”è®°è¿›è¡Œå››ä¸ªç»´åº¦çš„æ·±åº¦åˆ†æï¼š
1. å†…å®¹ç»“æ„åˆ†æ - æ ‡é¢˜ã€å¼€å¤´ã€æ­£æ–‡ã€ç»“å°¾
2. æƒ…æ„Ÿä»·å€¼åˆ†æ - ç—›ç‚¹ã€ä»·å€¼ä¸»å¼ ã€æƒ…æ„Ÿè§¦å‘
3. è§†è§‰å…ƒç´ åˆ†æ - å›¾ç‰‡é£æ ¼ã€è‰²å½©ã€æ’ç‰ˆ  
4. äº’åŠ¨æœºåˆ¶åˆ†æ - è¯„è®ºå¼•å¯¼ã€åˆ†äº«æœºåˆ¶ã€ç¤¾ç¾¤å»ºè®¾
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from crewai import Agent, Crew, Task, Process, LLM
from crewai.project import CrewBase, agent, crew, task
from langchain_openai import ChatOpenAI


# ä»å…¬å…±æ¨¡å‹å¯¼å…¥æ‰€æœ‰åˆ†æç›¸å…³ç±»å‹
from xhs_note_analyzer.models import (
    ContentStructureAnalysis,
    EmotionalValueAnalysis,
    VisualElementAnalysis,
    ContentAnalysisResult, 
    ContentAnalysisReport,
    PatternSynthesisResult
)

# å¯¼å…¥å…¬å…±æ•°æ®æ¨¡å‹
from xhs_note_analyzer.models import NoteContentData

logger = logging.getLogger(__name__)

@CrewBase
class ContentAnalyzerCrew():
    """å†…å®¹åˆ†æCrew"""

    agents_config = 'config/agents.yaml'
    tasks_config = 'config/tasks.yaml'

    def __init__(self):

        self.llm = ChatOpenAI(
        base_url='https://openrouter.ai/api/v1',
        model='openrouter/google/gemini-2.5-flash-lite',
        api_key=os.environ['OPENROUTER_API_KEY'],
        temperature=0.1
        )

        
    @agent
    def content_structure_analyst(self) -> Agent:
        """å†…å®¹ç»“æ„åˆ†æä¸“å®¶"""
        return Agent(
            config=self.agents_config['content_structure_analyst'],
            llm=self.llm,
            verbose=True
        )

    @agent 
    def emotional_value_analyst(self) -> Agent:
        """æƒ…æ„Ÿä»·å€¼åˆ†æä¸“å®¶"""
        return Agent(
            config=self.agents_config['emotional_value_analyst'],
            llm=self.llm,
            verbose=True
        )

    @agent
    def visual_element_analyst(self) -> Agent:
        """è§†è§‰å…ƒç´ åˆ†æä¸“å®¶"""
        return Agent(
            config=self.agents_config['visual_element_analyst'],
            llm=self.llm,
            verbose=True
        )


    @agent
    def content_analysis_coordinator(self) -> Agent:
        """å†…å®¹åˆ†æåè°ƒä¸“å®¶"""
        return Agent(
            config=self.agents_config['content_analysis_coordinator'],
            llm=self.llm,
            verbose=True
        )

    @agent
    def pattern_synthesis_analyst(self) -> Agent:
        """æ™ºèƒ½æ¨¡å¼åˆæˆä¸æˆåŠŸå…¬å¼æå–ä¸“å®¶"""
        return Agent(
            config=self.agents_config['pattern_synthesis_analyst'],
            llm=self.llm,
            verbose=True
        )

    @task
    def analyze_content_structure_task(self) -> Task:
        """å†…å®¹ç»“æ„åˆ†æä»»åŠ¡"""
        return Task(
            config=self.tasks_config['analyze_content_structure'],
            agent=self.content_structure_analyst(),
            output_pydantic=ContentStructureAnalysis
        )

    @task
    def analyze_emotional_value_task(self) -> Task:
        """æƒ…æ„Ÿä»·å€¼åˆ†æä»»åŠ¡"""
        return Task(
            config=self.tasks_config['analyze_emotional_value'],
            agent=self.emotional_value_analyst(),
            output_pydantic=EmotionalValueAnalysis
        )

    @task
    def analyze_visual_elements_task(self) -> Task:
        """è§†è§‰å…ƒç´ åˆ†æä»»åŠ¡"""
        return Task(
            config=self.tasks_config['analyze_visual_elements'],
            agent=self.visual_element_analyst(),
            output_pydantic=VisualElementAnalysis
        )


    @task
    def coordinate_content_analysis_task(self) -> Task:
        """å†…å®¹åˆ†æåè°ƒä»»åŠ¡"""
        return Task(
            config=self.tasks_config['coordinate_content_analysis'],
            agent=self.content_analysis_coordinator(),
            context=[
                self.analyze_content_structure_task(),
                self.analyze_emotional_value_task(),
                self.analyze_visual_elements_task()
            ]
            # åè°ƒä»»åŠ¡ä¸éœ€è¦ç‰¹å®šçš„Pydanticè¾“å‡ºï¼Œè¿”å›MarkdownæŠ¥å‘Šå³å¯
        )

    @task
    def synthesize_patterns_and_formulas_task(self) -> Task:
        """æ™ºèƒ½æ¨¡å¼åˆæˆä¸æˆåŠŸå…¬å¼æå–ä»»åŠ¡"""
        return Task(
            config=self.tasks_config['synthesize_patterns_and_formulas'],
            agent=self.pattern_synthesis_analyst(),
            output_pydantic=PatternSynthesisResult
        )

    @crew
    def crew(self) -> Crew:
        """åˆ›å»ºå†…å®¹åˆ†æCrew"""
        return Crew(
            agents=[
                self.content_structure_analyst(),
                self.emotional_value_analyst(),
                self.visual_element_analyst(),
                self.content_analysis_coordinator()
            ],
            tasks=[
                self.analyze_content_structure_task(),
                self.analyze_emotional_value_task(),
                self.analyze_visual_elements_task(),
                self.coordinate_content_analysis_task()
            ],
            process=Process.sequential,
            verbose=True
        )

    def analyze_single_note(self, note_data: NoteContentData) -> ContentAnalysisResult:
        """
        åˆ†æå•ä¸ªç¬”è®°
        
        Args:
            note_data: NoteContentDataå¯¹è±¡ï¼ŒåŒ…å«ç¬”è®°çš„è¯¦ç»†ä¿¡æ¯
            
        Returns:
            ContentAnalysisResult: åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹åˆ†æç¬”è®°: {note_data.note_id} - {note_data.title}")
            
            # å‡†å¤‡åˆ†ææ•°æ®
            analysis_input = {
                "note_id": note_data.note_id,
                "note_title": note_data.title,
                "note_content": note_data.content,
                "note_images": note_data.images,
                "image_count": len(note_data.images),
                "note_tags": note_data.tags,
                "author_info": note_data.author_info,
                "like_count": getattr(note_data.basic_info, 'like', 0),
                "collect_count": getattr(note_data.basic_info, 'collect', 0),
                "comment_count": getattr(note_data.basic_info, 'comment', 0)
            }
            
            # æ‰§è¡Œåˆ†æ
            result = self.crew().kickoff(inputs=analysis_input)
            
            # è§£æç»“æœå¹¶è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
            analysis_result = self._parse_analysis_result(result, note_data)
            
            logger.info(f"âœ… ç¬”è®°åˆ†æå®Œæˆ: {note_data.note_id}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ ç¬”è®°åˆ†æå¤±è´¥: {note_data.note_id}, é”™è¯¯: {e}")
            # è¿”å›åŸºç¡€åˆ†æç»“æœ
            return self._create_fallback_analysis(note_data)

    def analyze_multiple_notes(self, notes_data: List[NoteContentData]) -> ContentAnalysisReport:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªç¬”è®°ï¼Œå¹¶ä½¿ç”¨LLMæ™ºèƒ½æå–å…±åŒæ¨¡å¼å’ŒæˆåŠŸå…¬å¼
        
        Args:
            notes_data: List[NoteContentData] ç¬”è®°æ•°æ®åˆ—è¡¨
            
        Returns:
            ContentAnalysisReport: åŒ…å«æ™ºèƒ½æ¨¡å¼åˆ†æçš„å®Œæ•´åˆ†ææŠ¥å‘Š
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ {len(notes_data)} ä¸ªç¬”è®°")
            
            # Step 1: åˆ†ææ¯ä¸ªç¬”è®°
            analysis_results = []
            for note_data in notes_data:
                result = self.analyze_single_note(note_data)
                analysis_results.append(result)
            
            logger.info(f"ğŸ“Š å•ç¬”è®°åˆ†æå®Œæˆï¼Œå¼€å§‹æ™ºèƒ½æ¨¡å¼åˆæˆ")
            
            # Step 2: ä½¿ç”¨LLMæ™ºèƒ½æå–æ¨¡å¼å’Œå…¬å¼
            pattern_synthesis_result = self._synthesize_patterns_with_llm(analysis_results)
            
            # Step 3: ç”Ÿæˆæ•´åˆäº†æ™ºèƒ½åˆ†æçš„ç»¼åˆæŠ¥å‘Š
            report = self._generate_enhanced_analysis_report(analysis_results, pattern_synthesis_result)
            
            logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(analysis_results)} ä¸ªç¬”è®°ï¼Œæ™ºèƒ½æå–äº† {len(pattern_synthesis_result.success_formulas)} ä¸ªæˆåŠŸå…¬å¼")
            return report
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            raise

    def _parse_analysis_result(self, crew_result, note_data) -> ContentAnalysisResult:
        """è§£æCrewæ‰§è¡Œç»“æœå¹¶è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®"""
        try:
            logger.info(f"ğŸ” å¼€å§‹è§£æåˆ†æç»“æœï¼Œç±»å‹: {type(crew_result)}")
            
            # åˆå§‹åŒ–å„ç»´åº¦åˆ†æç»“æœ
            structure_analysis = None
            emotional_analysis = None
            visual_analysis = None
            
            # ä»tasks_outputä¸­è·å–å„ä¸ªä»»åŠ¡çš„Pydanticç»“æœ
            if hasattr(crew_result, 'tasks_output') and crew_result.tasks_output:
                logger.info(f"ğŸ“‹ ä»tasks_outputè§£æï¼Œä»»åŠ¡æ•°é‡: {len(crew_result.tasks_output)}")
                
                for i, task_output in enumerate(crew_result.tasks_output):
                    try:
                        logger.info(f"ğŸ”§ å¤„ç†ä»»åŠ¡ {i+1}: {type(task_output)}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰pydanticè¾“å‡ºï¼ˆè¿™æ˜¯æˆ‘ä»¬é…ç½®çš„output_pydanticï¼‰
                        if hasattr(task_output, 'pydantic') and task_output.pydantic:
                            pydantic_obj = task_output.pydantic
                            logger.info(f"âœ… ä»»åŠ¡ {i+1} æœ‰Pydanticè¾“å‡º: {type(pydantic_obj)}")
                            
                            # æ ¹æ®Pydanticå¯¹è±¡ç±»å‹ç¡®å®šæ˜¯å“ªä¸ªç»´åº¦çš„åˆ†æ
                            if isinstance(pydantic_obj, ContentStructureAnalysis):
                                structure_analysis = pydantic_obj
                                logger.info(f"âœ… è·å–åˆ°å†…å®¹ç»“æ„åˆ†æç»“æœ")
                            elif isinstance(pydantic_obj, EmotionalValueAnalysis):
                                emotional_analysis = pydantic_obj
                                logger.info(f"âœ… è·å–åˆ°æƒ…æ„Ÿä»·å€¼åˆ†æç»“æœ")
                            elif isinstance(pydantic_obj, VisualElementAnalysis):
                                visual_analysis = pydantic_obj
                                logger.info(f"âœ… è·å–åˆ°è§†è§‰å…ƒç´ åˆ†æç»“æœ")
                            else:
                                logger.warning(f"âš ï¸ æœªçŸ¥çš„Pydanticå¯¹è±¡ç±»å‹: {type(pydantic_obj)}")
                        
                        # å¦‚æœæ²¡æœ‰Pydanticè¾“å‡ºï¼Œå°è¯•ä»json_dictè·å–
                        elif hasattr(task_output, 'json_dict') and task_output.json_dict:
                            json_data = task_output.json_dict
                            logger.info(f"ğŸ“Š ä»»åŠ¡ {i+1} æœ‰JSONè¾“å‡ºï¼Œé”®: {list(json_data.keys())}")
                            
                            # æ ¹æ®ä»»åŠ¡é¡ºåºå’Œå†…å®¹åˆ¤æ–­ç±»å‹
                            if i == 0:  # ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯å†…å®¹ç»“æ„åˆ†æ
                                structure_analysis = ContentStructureAnalysis(note_id=note_data.note_id, **json_data)
                                logger.info(f"âœ… ä»JSONåˆ›å»ºå†…å®¹ç»“æ„åˆ†æç»“æœ")
                            elif i == 1:  # ç¬¬äºŒä¸ªä»»åŠ¡æ˜¯æƒ…æ„Ÿä»·å€¼åˆ†æ
                                emotional_analysis = EmotionalValueAnalysis(note_id=note_data.note_id, **json_data)
                                logger.info(f"âœ… ä»JSONåˆ›å»ºæƒ…æ„Ÿä»·å€¼åˆ†æç»“æœ")
                            elif i == 2:  # ç¬¬ä¸‰ä¸ªä»»åŠ¡æ˜¯è§†è§‰å…ƒç´ åˆ†æ
                                visual_analysis = VisualElementAnalysis(note_id=note_data.note_id, **json_data)
                                logger.info(f"âœ… ä»JSONåˆ›å»ºè§†è§‰å…ƒç´ åˆ†æç»“æœ")
                        
                        else:
                            logger.warning(f"âš ï¸ ä»»åŠ¡ {i+1} æ²¡æœ‰ç»“æ„åŒ–è¾“å‡ºï¼Œè·³è¿‡")
                            
                    except Exception as task_error:
                        logger.warning(f"âš ï¸ è§£æä»»åŠ¡ {i+1} å¤±è´¥: {task_error}")
                        continue
            
            # å¦‚æœæŸäº›ç»´åº¦çš„åˆ†æç»“æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤å€¼
            if not structure_analysis:
                logger.warning("âš ï¸ å†…å®¹ç»“æ„åˆ†æç»“æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤å€¼")
                structure_analysis = ContentStructureAnalysis(note_id=note_data.note_id)
            
            if not emotional_analysis:
                logger.warning("âš ï¸ æƒ…æ„Ÿä»·å€¼åˆ†æç»“æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤å€¼")
                emotional_analysis = EmotionalValueAnalysis(note_id=note_data.note_id)
            
            if not visual_analysis:
                logger.warning("âš ï¸ è§†è§‰å…ƒç´ åˆ†æç»“æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤å€¼")
                visual_analysis = VisualElementAnalysis(note_id=note_data.note_id)
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆåŸºäºå„ç»´åº¦è¯„åˆ†çš„å¹³å‡å€¼ï¼‰
            scores = []
            if hasattr(structure_analysis, 'readability_score') and structure_analysis.readability_score > 0:
                scores.append(structure_analysis.readability_score)
            if hasattr(emotional_analysis, 'emotional_intensity') and emotional_analysis.emotional_intensity > 0:
                scores.append(emotional_analysis.emotional_intensity)
            # è§†è§‰åˆ†ææ²¡æœ‰ç›´æ¥çš„è¯„åˆ†å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼
            if len(scores) == 0:
                overall_score = 75.0
            else:
                overall_score = sum(scores) / len(scores)
            
            # æ”¶é›†æˆåŠŸè¦ç´ 
            success_factors = []
            if structure_analysis.title_pattern:
                success_factors.append(f"æ ‡é¢˜ç­–ç•¥: {structure_analysis.title_pattern}")
            if emotional_analysis.pain_points:
                success_factors.append(f"ç—›ç‚¹æŒ–æ˜: {len(emotional_analysis.pain_points)}ä¸ªç—›ç‚¹")
            if visual_analysis.image_style:
                success_factors.append(f"è§†è§‰é£æ ¼: {visual_analysis.image_style}")
            
            if not success_factors:
                success_factors = ["å®ŒæˆåŸºç¡€åˆ†æ"]
            
            # åˆ›å»ºå®Œæ•´åˆ†æç»“æœ
            analysis_result = ContentAnalysisResult(
                note_id=note_data.note_id,
                note_title=note_data.title,
                structure_analysis=structure_analysis,
                emotional_analysis=emotional_analysis,
                visual_analysis=visual_analysis,
                overall_score=overall_score,
                success_factors=success_factors,
                improvement_suggestions=["åŸºäºå¤šç»´åº¦åˆ†æçš„ä¼˜åŒ–å»ºè®®"],
                replicability_score=overall_score * 0.9,  # å¯å¤åˆ¶æ€§ç•¥ä½äºæ•´ä½“è¯„åˆ†
                analysis_timestamp=datetime.now().isoformat(),
                analysis_version="1.0"
            )
            
            logger.info(f"âœ… æˆåŠŸè§£æåˆ†æç»“æœï¼Œç»¼åˆè¯„åˆ†: {overall_score:.1f}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ è§£æåˆ†æç»“æœå¤±è´¥: {e}")
            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            logger.error(f"âŒ å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
            return self._create_fallback_analysis(note_data)

    def _create_fallback_analysis(self, note_data: NoteContentData) -> ContentAnalysisResult:
        """åˆ›å»ºå¤‡ç”¨åˆ†æç»“æœ"""
        return ContentAnalysisResult(
            note_id=note_data.note_id,
            note_title=note_data.title,
            structure_analysis=ContentStructureAnalysis(note_id=note_data.note_id),
            emotional_analysis=EmotionalValueAnalysis(note_id=note_data.note_id),
            visual_analysis=VisualElementAnalysis(note_id=note_data.note_id),
            overall_score=60.0,
            success_factors=["åŸºç¡€å†…å®¹å®Œæ•´"],
            improvement_suggestions=["éœ€è¦æ·±åº¦åˆ†æä¼˜åŒ–"],
            analysis_timestamp=datetime.now().isoformat()
        )

    def _synthesize_patterns_with_llm(self, analysis_results: List[ContentAnalysisResult]) -> PatternSynthesisResult:
        """ä½¿ç”¨LLMæ™ºèƒ½åˆæˆæ¨¡å¼å’Œæå–æˆåŠŸå…¬å¼"""
        try:
            logger.info(f"ğŸ§  å¼€å§‹LLMæ™ºèƒ½æ¨¡å¼åˆæˆï¼Œè¾“å…¥ {len(analysis_results)} ä¸ªåˆ†æç»“æœ")
            
            # å‡†å¤‡åˆ†æè¾“å…¥æ•°æ®
            total_notes = len(analysis_results)
            average_score = sum(r.overall_score for r in analysis_results) / total_notes if total_notes > 0 else 0.0
            high_score_notes = [r for r in analysis_results if r.overall_score >= 80.0]
            high_score_count = len(high_score_notes)
            
            # æ„å»ºè¾“å…¥æ•°æ®
            synthesis_input = {
                "analysis_results": [result.model_dump() for result in analysis_results],
                "total_notes": total_notes,
                "average_score": average_score,
                "high_score_count": high_score_count
            }
            
            # åˆ›å»ºä¸“é—¨çš„æ¨¡å¼åˆæˆCrewæ¥æ‰§è¡ŒLLMåˆ†æ
            pattern_crew = Crew(
                agents=[self.pattern_synthesis_analyst()],
                tasks=[self.synthesize_patterns_and_formulas_task()],
                process=Process.sequential,
                verbose=True
            )
            
            # æ‰§è¡ŒLLMæ™ºèƒ½åˆ†æ
            pattern_task_result = pattern_crew.kickoff(inputs=synthesis_input)
            
            # è§£æLLMåˆ†æç»“æœ
            if hasattr(pattern_task_result, 'pydantic') and pattern_task_result.pydantic:
                pattern_result = pattern_task_result.pydantic
            elif hasattr(pattern_task_result, 'json_dict') and pattern_task_result.json_dict:
                pattern_result = PatternSynthesisResult(**pattern_task_result.json_dict)
            else:
                # å¦‚æœLLMä»»åŠ¡å¤±è´¥ï¼Œåˆ›å»ºåŸºç¡€ç»“æœ
                logger.warning("âš ï¸ LLMæ¨¡å¼åˆæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆæˆç»“æœ")
                pattern_result = self._create_basic_pattern_synthesis(analysis_results)
            
            return pattern_result
            
        except Exception as e:
            logger.error(f"âŒ LLMæ¨¡å¼åˆæˆå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€åˆæˆç»“æœ
            return self._create_basic_pattern_synthesis(analysis_results)

    def _create_basic_pattern_synthesis(self, analysis_results: List[ContentAnalysisResult]) -> PatternSynthesisResult:
        """åˆ›å»ºåŸºç¡€çš„æ¨¡å¼åˆæˆç»“æœï¼ˆä½œä¸ºLLMåˆ†æçš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        high_score_notes = [r for r in analysis_results if r.overall_score >= 80.0]
        
        return PatternSynthesisResult(
            common_patterns={
                "ç»“æ„æ¨¡å¼": ["åŸºç¡€ç»“æ„åˆ†æ", "æ ‡å‡†å¼€å¤´ç»“å°¾"],
                "æƒ…æ„Ÿæ¨¡å¼": ["åŸºç¡€æƒ…æ„Ÿè§¦å‘", "ä»·å€¼ä¸»å¼ è¯†åˆ«"],
                "è§†è§‰æ¨¡å¼": ["åŸºç¡€è§†è§‰é£æ ¼", "è‰²å½©æ­é…"]
            },
            success_formulas=[
                "é«˜è´¨é‡å†…å®¹ + æƒ…æ„Ÿå…±é¸£ = é«˜äº’åŠ¨",
                "æ¸…æ™°ç»“æ„ + è§†è§‰å¸å¼• = ç”¨æˆ·åœç•™"
            ],
            pattern_insights={
                "åŸºç¡€æ´å¯Ÿ": "éœ€è¦LLMæ·±åº¦åˆ†æè·å¾—æ›´å‡†ç¡®çš„æ´å¯Ÿ"
            },
            success_mechanisms=["å†…å®¹è´¨é‡", "ç”¨æˆ·ä½“éªŒ", "æƒ…æ„Ÿè¿æ¥"],
            replication_strategies=["ä¿æŒå†…å®¹è´¨é‡", "æ³¨é‡è§†è§‰è®¾è®¡", "å»ºç«‹æƒ…æ„Ÿè¿æ¥"],
            analysis_timestamp=datetime.now().isoformat(),
        )

    def _generate_enhanced_analysis_report(self, analysis_results: List[ContentAnalysisResult], 
                                         pattern_synthesis: PatternSynthesisResult) -> ContentAnalysisReport:
        """ç”Ÿæˆæ•´åˆäº†æ™ºèƒ½æ¨¡å¼åˆ†æçš„å¢å¼ºåˆ†ææŠ¥å‘Š"""
        total_notes = len(analysis_results)
        average_score = sum(r.overall_score for r in analysis_results) / total_notes if total_notes > 0 else 0.0
        
        report = ContentAnalysisReport(
            analysis_results=analysis_results,
            total_notes=total_notes,
            average_score=average_score,
            # å¤šç¯‡ç¬”è®°å…±åŒæ¨¡å¼åˆ†æ
            common_patterns=pattern_synthesis.common_patterns,
            success_formulas=pattern_synthesis.success_formulas,
            pattern_insights=pattern_synthesis.pattern_insights,
            success_mechanisms=pattern_synthesis.success_mechanisms,
            replication_strategies=pattern_synthesis.replication_strategies,
            report_timestamp=datetime.now().isoformat(),
            report_summary=f"å®Œæˆå¯¹{total_notes}ç¯‡ç¬”è®°çš„æ™ºèƒ½åŒ–å¤šç»´åº¦åˆ†æï¼Œå¹³å‡è¯„åˆ†{average_score:.1f}ï¼ŒLLMè¯†åˆ«{len(pattern_synthesis.success_formulas)}ä¸ªæˆåŠŸå…¬å¼"
        )
        
        return report

    def _generate_analysis_report(self, analysis_results: List[ContentAnalysisResult]) -> ContentAnalysisReport:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨æ™ºèƒ½åŒ–çš„analyze_multiple_notesï¼‰"""
        logger.info("ğŸ“Š ä½¿ç”¨åŸºç¡€æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨analyze_multiple_notesè·å¾—æ™ºèƒ½åŒ–åˆ†æ")
        
        total_notes = len(analysis_results)
        average_score = sum(r.overall_score for r in analysis_results) / total_notes if total_notes > 0 else 0.0
        
        # ä½¿ç”¨åŸºç¡€çš„æ¨¡å¼æå–ï¼ˆä¸å†ä¾èµ–æœºæ¢°åŒ–æ–¹æ³•ï¼‰
        common_patterns = {
            "æ ‡é¢˜æ¨¡å¼": ["å¾…æ™ºèƒ½åˆ†æ", "ä½¿ç”¨analyze_multiple_notesè·å¾—è¯¦ç»†æ¨¡å¼"],
            "å¼€å¤´ç­–ç•¥": ["å¾…æ™ºèƒ½åˆ†æ", "ä½¿ç”¨analyze_multiple_notesè·å¾—è¯¦ç»†ç­–ç•¥"],
            "è§†è§‰é£æ ¼": ["å¾…æ™ºèƒ½åˆ†æ", "ä½¿ç”¨analyze_multiple_notesè·å¾—è¯¦ç»†åˆ†æ"],
            "äº’åŠ¨æŠ€å·§": ["å¾…æ™ºèƒ½åˆ†æ", "æ¨èä½¿ç”¨æ™ºèƒ½åŒ–åˆ†ææ–¹æ³•"]
        }
        
        success_formulas = [
            "ä½¿ç”¨analyze_multiple_notesæ–¹æ³•è·å¾—LLMæ™ºèƒ½æå–çš„æˆåŠŸå…¬å¼",
            "åŸºç¡€åˆ†æå»ºè®®ï¼šä¿æŒå†…å®¹è´¨é‡ + å…³æ³¨ç”¨æˆ·ä½“éªŒ"
        ]
        
        report = ContentAnalysisReport(
            analysis_results=analysis_results,
            total_notes=total_notes,
            average_score=average_score,
            common_patterns=common_patterns,
            success_formulas=success_formulas,
            report_timestamp=datetime.now().isoformat(),
            report_summary=f"å®Œæˆå¯¹{total_notes}ç¯‡ç¬”è®°çš„åŸºç¡€å¤šç»´åº¦åˆ†æï¼Œå¹³å‡è¯„åˆ†{average_score:.1f}ã€‚æ¨èä½¿ç”¨analyze_multiple_notesè·å¾—æ™ºèƒ½åŒ–æ·±åº¦åˆ†æã€‚"
        )
        
        return report

    def save_analysis_results(self, analysis_results, output_dir: str = "output"):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        try:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
            
            if isinstance(analysis_results, ContentAnalysisReport):
                # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
                json_file = output_path / "content_analysis_results.json"
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis_results.model_dump(), f, ensure_ascii=False, indent=2)
                
                # ä¿å­˜Markdownæ ¼å¼çš„æŠ¥å‘Š
                markdown_file = output_path / "content_analysis_report.md"
                self._save_markdown_report(analysis_results, markdown_file)
                
                # ä¿å­˜ç®€è¦æ–‡æœ¬æ‘˜è¦
                summary_file = output_path / "content_analysis_summary.txt"
                with open(summary_file, 'w', encoding='utf-8') as f:
                    f.write(f"å°çº¢ä¹¦å†…å®¹æ™ºèƒ½åˆ†ææŠ¥å‘Š\\n")
                    f.write("=" * 50 + "\\n\\n")
                    f.write(f"åˆ†æç¬”è®°æ•°: {analysis_results.total_notes}\\n")
                    f.write(f"å¹³å‡è¯„åˆ†: {analysis_results.average_score:.1f}\\n")
                    f.write(f"ç”Ÿæˆæ—¶é—´: {analysis_results.report_timestamp}\\n")
                    f.write(f"æŠ¥å‘Šæ‘˜è¦: {analysis_results.report_summary}\\n\\n")
                    
                    # LLMè¯†åˆ«çš„å…±åŒæ¨¡å¼
                    f.write("ğŸ¤– LLMæ™ºèƒ½è¯†åˆ«çš„å…±åŒæ¨¡å¼:\\n")
                    for pattern_type, patterns in analysis_results.common_patterns.items():
                        f.write(f"  {pattern_type}: {', '.join(patterns)}\\n")
                    
                    # LLMæå–çš„æˆåŠŸå…¬å¼
                    f.write("\\nğŸ¯ LLMæ™ºèƒ½æå–çš„æˆåŠŸå…¬å¼:\\n")
                    for i, formula in enumerate(analysis_results.success_formulas, 1):
                        f.write(f"  {i}. {formula}\\n")
                    
                    # æ¨¡å¼æ´å¯Ÿï¼ˆæ–°å¢ï¼‰
                    if hasattr(analysis_results, 'pattern_insights') and analysis_results.pattern_insights:
                        f.write("\\nğŸ’¡ æ·±åº¦æ´å¯Ÿåˆ†æ:\\n")
                        for insight_key, insight_value in analysis_results.pattern_insights.items():
                            f.write(f"  {insight_key}: {insight_value}\\n")
                    
                    # æˆåŠŸæœºåˆ¶ï¼ˆæ–°å¢ï¼‰
                    if hasattr(analysis_results, 'success_mechanisms') and analysis_results.success_mechanisms:
                        f.write("\\nâš™ï¸ åº•å±‚æˆåŠŸæœºåˆ¶:\\n")
                        for i, mechanism in enumerate(analysis_results.success_mechanisms, 1):
                            f.write(f"  {i}. {mechanism}\\n")
                    
                    # å¤åˆ¶ç­–ç•¥ï¼ˆæ–°å¢ï¼‰
                    if hasattr(analysis_results, 'replication_strategies') and analysis_results.replication_strategies:
                        f.write("\\nğŸ“‹ å¯æ“ä½œçš„å¤åˆ¶ç­–ç•¥:\\n")
                        for i, strategy in enumerate(analysis_results.replication_strategies, 1):
                            f.write(f"  {i}. {strategy}\\n")
                
                print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_file}")
                print(f"ğŸ“‹ MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {markdown_file}")
                print(f"ğŸ“„ æ–‡æœ¬æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
                
            else:
                # ä¿å­˜å•ä¸ªåˆ†æç»“æœ
                results_file = output_path / "content_analysis_results.json"
                with open(results_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis_results.model_dump(), f, ensure_ascii=False, indent=2)
                print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {results_file}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

    def _save_markdown_report(self, report: ContentAnalysisReport, file_path: Path):
        """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
        with open(file_path, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜å’Œæ¦‚è§ˆ
            f.write("# å°çº¢ä¹¦å†…å®¹æ·±åº¦åˆ†ææŠ¥å‘Š\\n\\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {report.report_timestamp}\\n")
            f.write(f"**æŠ¥å‘Šæ‘˜è¦**: {report.report_summary}\\n\\n")
            
            # åˆ†ææ¦‚è§ˆ
            f.write("## ğŸ“Š æ™ºèƒ½åˆ†ææ¦‚è§ˆ\\n\\n")
            f.write(f"- **åˆ†æç¬”è®°æ•°**: {report.total_notes}\\n")
            f.write(f"- **å¹³å‡è¯„åˆ†**: {report.average_score:.1f}/100\\n")
            f.write(f"- **LLMè¯†åˆ«æˆåŠŸå…¬å¼**: {len(report.success_formulas)}\\n")
            f.write(f"- **LLMæå–å…±åŒæ¨¡å¼**: {len(report.common_patterns)}\\n")
            
            # æ™ºèƒ½åˆ†æé¢å¤–ç»Ÿè®¡ä¿¡æ¯
            if hasattr(report, 'pattern_insights') and report.pattern_insights:
                f.write(f"- **æ·±åº¦æ´å¯Ÿç»´åº¦**: {len(report.pattern_insights)}\\n")
            if hasattr(report, 'success_mechanisms') and report.success_mechanisms:
                f.write(f"- **è¯†åˆ«æˆåŠŸæœºåˆ¶**: {len(report.success_mechanisms)}\\n")
            if hasattr(report, 'replication_strategies') and report.replication_strategies:
                f.write(f"- **æä¾›å¤åˆ¶ç­–ç•¥**: {len(report.replication_strategies)}\\n")
            
            f.write("\\n> ğŸ¤– **é‡‡ç”¨LLMæ™ºèƒ½åˆ†ææŠ€æœ¯**ï¼Œæ·±åº¦æŒ–æ˜å†…å®¹æˆåŠŸè§„å¾‹ï¼Œæä¾›å¯æ“ä½œçš„ç­–ç•¥æŒ‡å¯¼\\n\\n")
            
            # LLMæ™ºèƒ½æå–çš„æˆåŠŸå…¬å¼
            if report.success_formulas:
                f.write("## ğŸ¯ LLMæ™ºèƒ½æå–çš„æˆåŠŸå…¬å¼\\n\\n")
                for i, formula in enumerate(report.success_formulas, 1):
                    f.write(f"{i}. **{formula}**\\n")
                f.write("\\n")
            
            # LLMè¯†åˆ«çš„å…±åŒæ¨¡å¼
            if report.common_patterns:
                f.write("## ğŸ” LLMæ™ºèƒ½è¯†åˆ«çš„å…±åŒæ¨¡å¼\\n\\n")
                for pattern_type, patterns in report.common_patterns.items():
                    f.write(f"### {pattern_type}\\n\\n")
                    for pattern in patterns:
                        f.write(f"- {pattern}\\n")
                    f.write("\\n")
            
            # æ·±åº¦æ´å¯Ÿåˆ†æï¼ˆæ–°å¢ï¼‰
            if hasattr(report, 'pattern_insights') and report.pattern_insights:
                f.write("## ğŸ’¡ æ·±åº¦æ´å¯Ÿåˆ†æ\\n\\n")
                for insight_key, insight_value in report.pattern_insights.items():
                    f.write(f"### {insight_key}\\n\\n")
                    f.write(f"{insight_value}\\n\\n")
            
            # åº•å±‚æˆåŠŸæœºåˆ¶ï¼ˆæ–°å¢ï¼‰
            if hasattr(report, 'success_mechanisms') and report.success_mechanisms:
                f.write("## âš™ï¸ åº•å±‚æˆåŠŸæœºåˆ¶\\n\\n")
                for i, mechanism in enumerate(report.success_mechanisms, 1):
                    f.write(f"{i}. **{mechanism}**\\n")
                f.write("\\n")
            
            # å¯æ“ä½œçš„å¤åˆ¶ç­–ç•¥ï¼ˆæ–°å¢ï¼‰
            if hasattr(report, 'replication_strategies') and report.replication_strategies:
                f.write("## ğŸ“‹ å¯æ“ä½œçš„å¤åˆ¶ç­–ç•¥\\n\\n")
                for i, strategy in enumerate(report.replication_strategies, 1):
                    f.write(f"{i}. **{strategy}**\\n")
                f.write("\\n")
            
            # è¯¦ç»†åˆ†æç»“æœ
            f.write("## ğŸ“‹ è¯¦ç»†åˆ†æç»“æœ\\n\\n")
            for i, result in enumerate(report.analysis_results, 1):
                f.write(f"### {i}. {result.note_title}\\n\\n")
                f.write(f"**ç¬”è®°ID**: `{result.note_id}`\\n")
                f.write(f"**ç»¼åˆè¯„åˆ†**: {result.overall_score:.1f}/100\\n")
                f.write(f"**å¯å¤åˆ¶æ€§**: {result.replicability_score:.1f}/100\\n\\n")
                
                # æˆåŠŸè¦ç´ 
                if result.success_factors:
                    f.write("**æˆåŠŸè¦ç´ **:\\n")
                    for factor in result.success_factors:
                        f.write(f"- {factor}\\n")
                    f.write("\\n")
                
                # æ”¹è¿›å»ºè®®  
                if result.improvement_suggestions:
                    f.write("**æ”¹è¿›å»ºè®®**:\\n")
                    for suggestion in result.improvement_suggestions:
                        f.write(f"- {suggestion}\\n")
                    f.write("\\n")
                
                # ä¸‰ä¸ªç»´åº¦çš„è¯¦ç»†åˆ†æ
                f.write("#### ğŸ“ å†…å®¹ç»“æ„åˆ†æ\\n\\n")
                sa = result.structure_analysis
                if sa.title_pattern:
                    f.write(f"- **æ ‡é¢˜æ¨¡å¼**: {sa.title_pattern}\\n")
                if sa.opening_strategy:
                    f.write(f"- **å¼€å¤´ç­–ç•¥**: {sa.opening_strategy}\\n")
                if sa.content_framework:
                    f.write(f"- **å†…å®¹æ¡†æ¶**: {sa.content_framework}\\n")
                if sa.ending_technique:
                    f.write(f"- **ç»“å°¾æŠ€å·§**: {sa.ending_technique}\\n")
                f.write("\\n")
                
                f.write("#### ğŸ’ æƒ…æ„Ÿä»·å€¼åˆ†æ\\n\\n")
                ea = result.emotional_analysis
                if ea.pain_points:
                    f.write(f"- **ç—›ç‚¹æŒ–æ˜**: {', '.join(ea.pain_points)}\\n")
                if ea.value_propositions:
                    f.write(f"- **ä»·å€¼ä¸»å¼ **: {', '.join(ea.value_propositions)}\\n")
                if ea.emotional_triggers:
                    f.write(f"- **æƒ…æ„Ÿè§¦å‘**: {', '.join(ea.emotional_triggers)}\\n")
                f.write("\\n")
                
                f.write("#### ğŸ¨ è§†è§‰å…ƒç´ åˆ†æ\\n\\n")
                va = result.visual_analysis
                if va.image_style:
                    f.write(f"- **å›¾ç‰‡é£æ ¼**: {va.image_style}\\n")
                if va.color_scheme:
                    f.write(f"- **è‰²å½©æ–¹æ¡ˆ**: {va.color_scheme}\\n")
                if va.layout_style:
                    f.write(f"- **æ’ç‰ˆé£æ ¼**: {va.layout_style}\\n")
                f.write("\\n")
                
                f.write("---\\n\\n")


def create_content_analyzer() -> ContentAnalyzerCrew:
    """åˆ›å»ºå†…å®¹åˆ†æå™¨å®ä¾‹"""
    return ContentAnalyzerCrew()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    analyzer = create_content_analyzer()
    print("âœ… ContentAnalyzerCrewåˆ›å»ºæˆåŠŸ")