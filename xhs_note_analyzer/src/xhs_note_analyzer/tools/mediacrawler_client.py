"""
MediaCrawler API å®¢æˆ·ç«¯å·¥å…·
ç”¨äºä¸MediaCrawleræœåŠ¡å™¨é€šä¿¡ï¼Œè·å–å°çº¢ä¹¦ç¬”è®°è¯¦ç»†å†…å®¹
å‚è€ƒ: https://github.com/BradLeon/MediaCrawler-API-Server
"""

import os
import json
import requests
# Removed unused imports: asyncio, aiohttp
import time
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
import logging

logger = logging.getLogger(__name__)


class MediaCrawlerClient:
    """MediaCrawler API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_endpoint: str = None, api_key: str = None, debug_requests: bool = True):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            api_endpoint: APIæœåŠ¡å™¨åœ°å€ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡MEDIACRAWLER_API_ENDPOINTè·å–
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡MEDIACRAWLER_API_KEYè·å–
            debug_requests: æ˜¯å¦å¼€å¯HTTPè¯·æ±‚è°ƒè¯•ï¼Œé»˜è®¤True
        """
        self.api_endpoint = api_endpoint or os.getenv("MEDIACRAWLER_API_ENDPOINT", "http://localhost:8000")
        self.api_key = api_key or os.getenv("MEDIACRAWLER_API_KEY", "")
        self.debug_requests = debug_requests
        self.session = requests.Session()
        
        # è®¾ç½®è®¤è¯å¤´
        if self.api_key:
            self.session.headers.update({"Authorization": f"Bearer {self.api_key}"})
            
        # è®¾ç½®é»˜è®¤å¤´éƒ¨
        self.session.headers.update({
            "Content-Type": "application/json",
            "User-Agent": "MediaCrawler-Client/1.0"
        })
        
        logger.info(f"ğŸ”§ MediaCrawlerå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ”— APIç«¯ç‚¹: {self.api_endpoint}")
        logger.info(f"ğŸ”‘ APIå¯†é’¥: {'å·²è®¾ç½®' if self.api_key else 'æœªè®¾ç½®'}")
        logger.info(f"ğŸ› è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if self.debug_requests else 'å…³é—­'}")
    
    def extract_note_id_from_url(self, note_url: str) -> Optional[str]:
        """ä»å°çº¢ä¹¦ç¬”è®°URLæå–note_id"""
        try:
            # å¦‚æœè¾“å…¥æœ¬èº«å°±æ˜¯note_idï¼ˆ24ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰ï¼Œç›´æ¥è¿”å›
            if len(note_url) == 24 and all(c in '0123456789abcdef' for c in note_url.lower()):
                return note_url
            
            # å°çº¢ä¹¦ç¬”è®°URLæ ¼å¼æ”¯æŒï¼š
            # 1. https://xiaohongshu.com/note/[note_id]
            # 2. https://www.xiaohongshu.com/note/[note_id]  
            # 3. https://www.xiaohongshu.com/explore/[note_id]?xsec_token=xxx&xsec_source=xxx
            # 4. https://xhslink.com/xxx (çŸ­é“¾æ¥ï¼Œæš‚ä¸æ”¯æŒ)
            
            parsed = urlparse(note_url)
            
            if 'xiaohongshu.com' in parsed.netloc:
                path = parsed.path
                
                # å¤„ç† /note/ è·¯å¾„
                if '/note/' in path:
                    note_id = path.split('/note/')[-1].split('?')[0].split('#')[0]
                    if note_id and len(note_id) >= 20:  # note_idé€šå¸¸æ˜¯24ä½ï¼Œä½†è‡³å°‘20ä½
                        return note_id
                
                # å¤„ç† /explore/ è·¯å¾„ (æ–°æ ¼å¼)
                elif '/explore/' in path:
                    note_id = path.split('/explore/')[-1].split('?')[0].split('#')[0]
                    if note_id and len(note_id) >= 20:  # note_idé€šå¸¸æ˜¯24ä½ï¼Œä½†è‡³å°‘20ä½
                        return note_id
                        
                # å¤„ç† /discovery/item/ è·¯å¾„ (å¦ä¸€ç§å¯èƒ½æ ¼å¼)
                elif '/discovery/item/' in path:
                    note_id = path.split('/discovery/item/')[-1].split('?')[0].split('#')[0]
                    if note_id and len(note_id) >= 20:
                        return note_id
            
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…24ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²
            import re
            note_id_pattern = r'[0-9a-f]{24}'
            match = re.search(note_id_pattern, note_url.lower())
            if match:
                return match.group(0)
            
            logger.warning(f"æ— æ³•ä»URLæå–note_id: {note_url}")
            return None
            
        except Exception as e:
            logger.error(f"è§£æURLå¤±è´¥: {note_url}, é”™è¯¯: {e}")
            return None
    
    def create_crawl_task(self, note_urls: List[str], fetch_comments: bool = False, 
                         max_comments: int = 100) -> Dict[str, Any]:
        """
        åˆ›å»ºå°çº¢ä¹¦å†…å®¹é‡‡é›†ä»»åŠ¡
        
        Args:
            note_urls: ç¬”è®°URLåˆ—è¡¨ï¼ˆå¿…é¡»åŒ…å«xsec_tokenå’Œxsec_sourceå‚æ•°ï¼‰
            fetch_comments: æ˜¯å¦è·å–è¯„è®º
            max_comments: æœ€å¤§è¯„è®ºæ•°é‡
            
        Returns:
            ä»»åŠ¡åˆ›å»ºç»“æœ
        """
        try:
            # ä»URLæå–note_idsç”¨äºcontent_idså­—æ®µ
            note_ids = []
            for url in note_urls:
                note_id = self.extract_note_id_from_url(url)
                if note_id:
                    note_ids.append(note_id)
                else:
                    logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–note_id: {url}")
            
            # æ„å»ºç¬¦åˆæ–°APIæ ¼å¼çš„payload
            payload = {
                "platform": "xhs",
                "task_type": "detail",
                "content_ids": note_ids,                    # æå–çš„note_idåˆ—è¡¨
                "xhs_note_urls": note_urls,                 # å¿…éœ€ï¼šåŒ…å«tokençš„å®Œæ•´URL
                "max_count": len(note_urls),
                "max_comments": max_comments if fetch_comments else 0,
                "start_page": 1,
                "enable_proxy": False,
                "headless": False,
                "enable_comments": fetch_comments,
                "enable_sub_comments": fetch_comments,
                "save_data_option": "db",
                "clear_cookies": False
            }
            
            logger.info(f"ğŸ”„ åˆ›å»ºé‡‡é›†ä»»åŠ¡ï¼Œç›®æ ‡æ•°é‡: {len(note_urls)}")
            logger.info(f"ğŸ“‹ URLæ ¼å¼: {len([url for url in note_urls if 'xsec_token' in url])}/{len(note_urls)} åŒ…å«token")
            
            # è°ƒè¯•: æ‰“å°å®Œæ•´è¯·æ±‚ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¡ POSTè¯·æ±‚: {self.api_endpoint}/api/v1/tasks")
                logger.info(f"ğŸ“¦ è¯·æ±‚å¤´: {dict(self.session.headers)}")
                logger.info(f"ğŸ“„ è¯·æ±‚ä½“: {json.dumps(payload, ensure_ascii=False, indent=2)}")
            
            response = self.session.post(
                f"{self.api_endpoint}/api/v1/tasks",
                json=payload,
                timeout=30
            )
            
            # è°ƒè¯•: æ‰“å°å“åº”ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¨ å“åº”çŠ¶æ€: {response.status_code} {response.reason}")
                logger.info(f"ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
                logger.info(f"ğŸ“ å“åº”ä½“: {response.text[:1000]}...")
            
            response.raise_for_status()
            
            result = response.json()
            # é€‚é…æ–°çš„CrawlerTaskResponseæ ¼å¼
            if result.get("task_id"):
                logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {result.get('task_id')}")
                logger.info(f"ğŸ“ æœåŠ¡å™¨æ¶ˆæ¯: {result.get('message', 'N/A')}")
                # ä¸ºäº†å…¼å®¹æ€§ï¼Œç¡®ä¿è¿”å›successæ ‡å¿—
                result["success"] = True
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡åˆ›å»ºå“åº”å¼‚å¸¸: {result}")
                result["success"] = False
                
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥, é”™è¯¯: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"âŒ å“åº”çŠ¶æ€: {e.response.status_code}")
                logger.error(f"âŒ å“åº”å†…å®¹: {e.response.text[:500]}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        try:
            url = f"{self.api_endpoint}/api/v1/tasks/{task_id}/status"
            
            # è°ƒè¯•: æ‰“å°è¯·æ±‚ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¡ GETè¯·æ±‚: {url}")
                logger.info(f"ğŸ“¦ è¯·æ±‚å¤´: {dict(self.session.headers)}")
            
            response = self.session.get(url, timeout=10)
            
            # è°ƒè¯•: æ‰“å°å“åº”ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¨ å“åº”çŠ¶æ€: {response.status_code} {response.reason}")
                logger.info(f"ğŸ“ å“åº”ä½“: {response.text[:500]}...")
            
            response.raise_for_status()
            
            result = response.json()
            status = result.get("status", "unknown")
            logger.info(f"ğŸ“Š ä»»åŠ¡ {task_id} çŠ¶æ€: {status}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id}, é”™è¯¯: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"âŒ å“åº”çŠ¶æ€: {e.response.status_code}")
                logger.error(f"âŒ å“åº”å†…å®¹: {e.response.text[:500]}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {task_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def get_task_result(self, task_id: str) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡æ‰§è¡Œç»“æœ
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            ä»»åŠ¡ç»“æœä¿¡æ¯
        """
        try:
            url = f"{self.api_endpoint}/api/v1/tasks/{task_id}/result"
            
            if self.debug_requests:
                logger.info(f"ğŸ“¡ GETè¯·æ±‚: {url}")
            
            response = self.session.get(url, timeout=10)
            
            if self.debug_requests:
                logger.info(f"ğŸ“¨ å“åº”çŠ¶æ€: {response.status_code} {response.reason}")
                logger.info(f"ğŸ“ å“åº”ä½“: {response.text[:1000]}...")
            
            response.raise_for_status()
            
            result = response.json()
            logger.info(f"ğŸ“Š ä»»åŠ¡ {task_id} ç»“æœ: æˆåŠŸ={result.get('success')}, æ•°æ®æ¡æ•°={result.get('data_count', 0)}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡ç»“æœå¤±è´¥: {task_id}, é”™è¯¯: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"âŒ å“åº”çŠ¶æ€: {e.response.status_code}")
                logger.error(f"âŒ å“åº”å†…å®¹: {e.response.text[:500]}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {task_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def get_note_content_by_id(self, note_id: str) -> Dict[str, Any]:
        """
        æ ¹æ®note_idè·å–ç¬”è®°å†…å®¹
        
        Args:
            note_id: ç¬”è®°ID
            
        Returns:
            ç¬”è®°è¯¦ç»†å†…å®¹
        """
        try:
            logger.info(f"ğŸ” æŸ¥è¯¢ç¬”è®°å†…å®¹: {note_id}")
            
            url = f"{self.api_endpoint}/api/v1/data/content/xhs/{note_id}"
            
            # è°ƒè¯•: æ‰“å°è¯·æ±‚ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¡ GETè¯·æ±‚: {url}")
                logger.info(f"ğŸ“¦ è¯·æ±‚å¤´: {dict(self.session.headers)}")
            
            response = self.session.get(url, timeout=10)
            
            # è°ƒè¯•: æ‰“å°å“åº”ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¨ å“åº”çŠ¶æ€: {response.status_code} {response.reason}")
                logger.info(f"ğŸ“ å“åº”ä½“: {response.text[:1000]}...")
            
            response.raise_for_status()
            
            result = response.json()
            
            if result.get("data"):
                logger.info(f"âœ… æˆåŠŸè·å–ç¬”è®°å†…å®¹: {note_id}")
                # ä¸ºäº†å…¼å®¹æ€§ï¼Œæ·»åŠ successå­—æ®µ
                result["success"] = True
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç¬”è®°å†…å®¹: {note_id}")
                result["success"] = False
            
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æŸ¥è¯¢ç¬”è®°å¤±è´¥: {note_id}, é”™è¯¯: {e}")
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"âŒ å“åº”çŠ¶æ€: {e.response.status_code}")
                logger.error(f"âŒ å“åº”å†…å®¹: {e.response.text[:500]}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {note_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}

    def wait_for_task_completion(self, task_id: str, max_wait_time: int = 600, 
                                check_interval: int = 5) -> bool:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆ
        
        Args:
            task_id: ä»»åŠ¡ID
            max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            status_result = self.get_task_status(task_id)
            
            if not status_result.get("success", True):
                # å¦‚æœè·å–çŠ¶æ€å¤±è´¥ï¼Œç­‰ä¸€ä¼šå†è¯•
                time.sleep(check_interval)
                continue
                
            status = status_result.get("status", "unknown")
            done = status_result.get("done", False)
            success = status_result.get("success")
            
            if done:
                if success is True:
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆ")
                    return True
                else:
                    logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥")
                    return False
            elif status == "failed":
                logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥")
                return False
            elif status in ["running", "pending"]:
                # ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œç»§ç»­ç­‰å¾…
                progress = status_result.get("progress", {})
                if progress:
                    percent = progress.get("progress_percent", 0)
                    stage = progress.get("current_stage", "æœªçŸ¥")
                    logger.info(f"â³ ä»»åŠ¡ {task_id} è¿›è¡Œä¸­: {stage} ({percent:.1f}%)")
                else:
                    logger.info(f"â³ ä»»åŠ¡ {task_id} çŠ¶æ€: {status}")
                time.sleep(check_interval)
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} çŠ¶æ€æœªçŸ¥: {status}")
                time.sleep(check_interval)
        
        logger.error(f"â° ä»»åŠ¡ {task_id} ç­‰å¾…è¶…æ—¶")
        return False

    def crawl_note(self, note_url: str, fetch_comments: bool = False) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªç¬”è®°å†…å®¹ï¼ˆé«˜å±‚æ¬¡æ¥å£ï¼Œè‡ªåŠ¨å¤„ç†ä»»åŠ¡åˆ›å»ºå’Œç­‰å¾…ï¼‰
        
        Args:
            note_url: ç¬”è®°URL
            fetch_comments: æ˜¯å¦è·å–è¯„è®º
            
        Returns:
            ç¬”è®°è¯¦ç»†å†…å®¹æ•°æ®
        """
        try:
            # ä»URLæå–note_id
            note_id = self.extract_note_id_from_url(note_url)
            if not note_id:
                raise ValueError(f"æ— æ³•ä»URLæå–note_id: {note_url}")
            
            logger.info(f"ğŸ”„ å¼€å§‹çˆ¬å–ç¬”è®°: {note_url} (ID: {note_id})")
            
            # å…ˆå°è¯•ä»æ•°æ®åº“è·å–
            existing_data = self.get_note_content_by_id(note_id)
            if existing_data.get("success") and existing_data.get("data"):
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–åˆ°ç°æœ‰æ•°æ®: {note_id}")
                return existing_data
            
            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºé‡‡é›†ä»»åŠ¡
            task_result = self.create_crawl_task(
                note_urls=[note_url],
                fetch_comments=fetch_comments
            )
            
            if not task_result.get("task_id"):
                raise Exception(f"ä»»åŠ¡åˆ›å»ºå¤±è´¥: {task_result.get('message', 'Unknown error')}")
            
            task_id = task_result["task_id"]
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            if self.wait_for_task_completion(task_id, max_wait_time=300):
                # ä»»åŠ¡å®Œæˆï¼Œè·å–ç»“æœ
                result = self.get_note_content_by_id(note_id)
                if result.get("success"):
                    logger.info(f"âœ… æˆåŠŸçˆ¬å–ç¬”è®°å†…å®¹: {note_url}")
                    return result
                else:
                    raise Exception(f"ä»»åŠ¡å®Œæˆä½†æ— æ³•è·å–æ•°æ®: {result.get('message')}")
            else:
                raise Exception(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥æˆ–è¶…æ—¶: {task_id}")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ç¬”è®°å¤±è´¥: {note_url}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def batch_crawl_notes(self, note_urls: List[str], fetch_comments: bool = False) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–ç¬”è®°å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å•ä¸ªä»»åŠ¡å¤„ç†å¤šä¸ªç¬”è®°ï¼‰
        
        Args:
            note_urls: ç¬”è®°URLåˆ—è¡¨
            fetch_comments: æ˜¯å¦è·å–è¯„è®º
            
        Returns:
            ç¬”è®°è¯¦ç»†å†…å®¹åˆ—è¡¨
        """
        if not note_urls:
            return []
        
        try:
            # æ„å»ºURLåˆ°note_idçš„æ˜ å°„ï¼ŒåŒæ—¶æ”¶é›†æœ‰æ•ˆçš„URLå’Œnote_id
            valid_urls = []
            url_to_id_map = {}  # URL -> note_id
            
            for url in note_urls:
                note_id = self.extract_note_id_from_url(url)
                if note_id:
                    valid_urls.append(url)
                    url_to_id_map[url] = note_id
                else:
                    logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–note_id: {url}")
            
            if not valid_urls:
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„URL")
                return [{"success": False, "error": "æ²¡æœ‰æœ‰æ•ˆçš„note_id"}] * len(note_urls)
            
            logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡çˆ¬å– {len(valid_urls)} ä¸ªç¬”è®°")
            
            # æ£€æŸ¥å“ªäº›æ•°æ®å·²å­˜åœ¨ï¼Œå“ªäº›éœ€è¦æ–°é‡‡é›†
            existing_data = {}
            new_urls = []
            
            for url in valid_urls:
                note_id = url_to_id_map[url]
                existing_result = self.get_note_content_by_id(note_id)
                if existing_result.get("success") and existing_result.get("data"):
                    existing_data[note_id] = existing_result
                    logger.info(f"âœ… ä»æ•°æ®åº“è·å–åˆ°ç°æœ‰æ•°æ®: {note_id}")
                else:
                    new_urls.append(url)
            
            # å¯¹äºéœ€è¦æ–°é‡‡é›†çš„URLï¼Œåˆ›å»ºæ‰¹é‡é‡‡é›†ä»»åŠ¡
            if new_urls:
                logger.info(f"ğŸš€ åˆ›å»ºæ‰¹é‡é‡‡é›†ä»»åŠ¡ï¼Œç›®æ ‡: {len(new_urls)} ä¸ªæ–°ç¬”è®°")
                
                task_result = self.create_crawl_task(
                    note_urls=new_urls,
                    fetch_comments=fetch_comments
                )
                
                if task_result.get("task_id"):
                    task_id = task_result["task_id"]
                    
                    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                    if self.wait_for_task_completion(task_id, max_wait_time=600):
                        logger.info(f"âœ… æ‰¹é‡é‡‡é›†ä»»åŠ¡å®Œæˆ: {task_id}")
                        
                        # è·å–æ–°é‡‡é›†çš„æ•°æ®
                        for url in new_urls:
                            note_id = url_to_id_map[url]
                            result = self.get_note_content_by_id(note_id)
                            if result.get("success"):
                                existing_data[note_id] = result
                            else:
                                existing_data[note_id] = {"success": False, "error": f"æ— æ³•è·å–æ•°æ®: {note_id}"}
                    else:
                        logger.error(f"âŒ æ‰¹é‡é‡‡é›†ä»»åŠ¡å¤±è´¥æˆ–è¶…æ—¶: {task_id}")
                        # ä¸ºå¤±è´¥çš„URLåˆ›å»ºé”™è¯¯ç»“æœ
                        for url in new_urls:
                            note_id = url_to_id_map[url]
                            existing_data[note_id] = {"success": False, "error": f"é‡‡é›†ä»»åŠ¡å¤±è´¥: {task_id}"}
                else:
                    logger.error(f"âŒ åˆ›å»ºæ‰¹é‡ä»»åŠ¡å¤±è´¥: {task_result.get('message')}")
                    # ä¸ºæ‰€æœ‰æ–°URLåˆ›å»ºé”™è¯¯ç»“æœ
                    for url in new_urls:
                        note_id = url_to_id_map[url]
                        existing_data[note_id] = {"success": False, "error": "åˆ›å»ºé‡‡é›†ä»»åŠ¡å¤±è´¥"}
            
            # æŒ‰åŸå§‹URLé¡ºåºç»„ç»‡ç»“æœ
            results = []
            for url in note_urls:
                if url in url_to_id_map:
                    note_id = url_to_id_map[url]
                    results.append(existing_data.get(note_id, {"success": False, "error": f"æœªå¤„ç†çš„ç¬”è®°: {note_id}"}))
                else:
                    results.append({"success": False, "error": f"æ— æ³•å¤„ç†URL: {url}"})
            
            success_count = len([r for r in results if r.get("success", False)])
            logger.info(f"ğŸ¯ æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}/{len(note_urls)}")
            logger.info(f"ğŸ“Š æ•°æ®æ¥æº: ç¼“å­˜ {len(existing_data) - len(new_urls if 'new_urls' in locals() else [])}, æ–°é‡‡é›† {len(new_urls if 'new_urls' in locals() else [])}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
            return [{"success": False, "error": str(e)}] * len(note_urls)
    
    def health_check(self) -> bool:
        """æ£€æŸ¥APIæœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            url = f"{self.api_endpoint}/api/v1/data/health"
            
            # è°ƒè¯•: æ‰“å°è¯·æ±‚ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¡ GETè¯·æ±‚: {url}")
                logger.info(f"ğŸ“¦ è¯·æ±‚å¤´: {dict(self.session.headers)}")
            
            response = self.session.get(url, timeout=5)
            
            # è°ƒè¯•: æ‰“å°å“åº”ä¿¡æ¯
            if self.debug_requests:
                logger.info(f"ğŸ“¨ å“åº”çŠ¶æ€: {response.status_code} {response.reason}")
                logger.info(f"ğŸ“ å“åº”ä½“: {response.text[:200]}...")
            
            is_healthy = response.status_code == 200
            logger.info(f"ğŸ©º å¥åº·æ£€æŸ¥ç»“æœ: {'âœ… å¥åº·' if is_healthy else 'âŒ ä¸å¥åº·'}")
            
            return is_healthy
            
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False


def create_mediacrawler_client(debug_requests: bool = True) -> MediaCrawlerClient:
    """åˆ›å»ºMediaCrawlerå®¢æˆ·ç«¯å®ä¾‹"""
    return MediaCrawlerClient(debug_requests=debug_requests)


# ä¾¿æ·å‡½æ•°
def fetch_note_content(note_url: str, fetch_comments: bool = False) -> Dict[str, Any]:
    """è·å–å•ä¸ªç¬”è®°å†…å®¹çš„ä¾¿æ·å‡½æ•°"""
    client = create_mediacrawler_client()
    return client.crawl_note(note_url, fetch_comments)


def batch_fetch_note_contents(note_urls: List[str], fetch_comments: bool = False) -> List[Dict[str, Any]]:
    """æ‰¹é‡è·å–ç¬”è®°å†…å®¹çš„ä¾¿æ·å‡½æ•°"""
    client = create_mediacrawler_client()
    return client.batch_crawl_notes(note_urls, fetch_comments)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import logging
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•APIè¿æ¥
    client = create_mediacrawler_client()
    
    print("ğŸ§ª æµ‹è¯•MediaCrawler APIè¿æ¥...")
    if client.health_check():
        print("âœ… APIæœåŠ¡å™¨è¿æ¥æ­£å¸¸")
        
        # å¦‚æœAPIå¯ç”¨ï¼Œæµ‹è¯•ç¬”è®°çˆ¬å–
        test_urls = [
            "https://www.xiaohongshu.com/explore/65728c2a000000003403fc88?xsec_token=ZBBMPkZKZC66wNYHvcBT26aYWhVGGRQZBgbpYzClweEPc=&xsec_source=pc_ad"
        ]
        print(f"\nğŸ§ª æµ‹è¯•ç¬”è®°çˆ¬å–: {test_urls[0][:80]}...")
        result = client.crawl_note(test_urls[0])
        print(f"ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}...")
    else:
        print("âŒ APIæœåŠ¡å™¨è¿æ¥å¤±è´¥ï¼Œè·³è¿‡çˆ¬å–æµ‹è¯•")