"""
MediaCrawler API å®¢æˆ·ç«¯å·¥å…·
ç”¨äºä¸MediaCrawleræœåŠ¡å™¨é€šä¿¡ï¼Œè·å–å°çº¢ä¹¦ç¬”è®°è¯¦ç»†å†…å®¹
å‚è€ƒ: https://github.com/BradLeon/MediaCrawler-API-Server
"""

import os
import json
import requests
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
import logging

logger = logging.getLogger(__name__)


class MediaCrawlerClient:
    """MediaCrawler API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_endpoint: str = None, api_key: str = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            api_endpoint: APIæœåŠ¡å™¨åœ°å€ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡MEDIACRAWLER_API_ENDPOINTè·å–
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡MEDIACRAWLER_API_KEYè·å–
        """
        self.api_endpoint = api_endpoint or os.getenv("MEDIACRAWLER_API_ENDPOINT", "http://localhost:8000")
        self.api_key = api_key or os.getenv("MEDIACRAWLER_API_KEY", "")
        self.session = requests.Session()
        
        # è®¾ç½®è®¤è¯å¤´
        if self.api_key:
            self.session.headers.update({"Authorization": f"Bearer {self.api_key}"})
    
    def extract_note_id_from_url(self, note_url: str) -> Optional[str]:
        """ä»å°çº¢ä¹¦ç¬”è®°URLæå–note_id"""
        try:
            # å¦‚æœè¾“å…¥æœ¬èº«å°±æ˜¯note_idï¼ˆ24ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰ï¼Œç›´æ¥è¿”å›
            if len(note_url) == 24 and all(c in '0123456789abcdef' for c in note_url.lower()):
                return note_url
            
            # å°çº¢ä¹¦ç¬”è®°URLæ ¼å¼æ”¯æŒï¼š
            # 1. https://xiaohongshu.com/note/[note_id]
            # 2. https://www.xiaohongshu.com/note/[note_id]  
            # 3. https://www.xiaohongshu.com/explore/[note_id]?xsec_token=xxx&xsec_source=xxx
            # 4. https://xhslink.com/xxx (çŸ­é“¾æ¥ï¼Œæš‚ä¸æ”¯æŒ)
            
            parsed = urlparse(note_url)
            
            if 'xiaohongshu.com' in parsed.netloc:
                path = parsed.path
                
                # å¤„ç† /note/ è·¯å¾„
                if '/note/' in path:
                    note_id = path.split('/note/')[-1].split('?')[0].split('#')[0]
                    if note_id and len(note_id) >= 20:  # note_idé€šå¸¸æ˜¯24ä½ï¼Œä½†è‡³å°‘20ä½
                        return note_id
                
                # å¤„ç† /explore/ è·¯å¾„ (æ–°æ ¼å¼)
                elif '/explore/' in path:
                    note_id = path.split('/explore/')[-1].split('?')[0].split('#')[0]
                    if note_id and len(note_id) >= 20:  # note_idé€šå¸¸æ˜¯24ä½ï¼Œä½†è‡³å°‘20ä½
                        return note_id
                        
                # å¤„ç† /discovery/item/ è·¯å¾„ (å¦ä¸€ç§å¯èƒ½æ ¼å¼)
                elif '/discovery/item/' in path:
                    note_id = path.split('/discovery/item/')[-1].split('?')[0].split('#')[0]
                    if note_id and len(note_id) >= 20:
                        return note_id
            
            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…24ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²
            import re
            note_id_pattern = r'[0-9a-f]{24}'
            match = re.search(note_id_pattern, note_url.lower())
            if match:
                return match.group(0)
            
            logger.warning(f"æ— æ³•ä»URLæå–note_id: {note_url}")
            return None
            
        except Exception as e:
            logger.error(f"è§£æURLå¤±è´¥: {note_url}, é”™è¯¯: {e}")
            return None
    
    def create_crawl_task(self, note_ids: List[str], task_id: str = None, fetch_comments: bool = False, 
                         max_comments: int = 100) -> Dict[str, Any]:
        """
        åˆ›å»ºå°çº¢ä¹¦å†…å®¹é‡‡é›†ä»»åŠ¡
        
        Args:
            note_ids: ç¬”è®°IDåˆ—è¡¨
            task_id: ä»»åŠ¡IDï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨ç”Ÿæˆ
            fetch_comments: æ˜¯å¦è·å–è¯„è®º
            max_comments: æœ€å¤§è¯„è®ºæ•°é‡
            
        Returns:
            ä»»åŠ¡åˆ›å»ºç»“æœ
        """
        try:
            import time
            if not task_id:
                task_id = f"crawl_task_{int(time.time())}"
            
            payload = {
                "task_id": task_id,
                "platform": "xhs",
                "task_type": "detail",
                "content_ids": note_ids,
                "max_count": len(note_ids),
                "max_comments": max_comments if fetch_comments else 0,
                "enable_proxy": False,
                "headless": True,
                "save_data_option": "db",
                "config": {
                    "login_type": "cookie",
                    "enable_stealth": True,
                    "random_sleep_min": 1.0,
                    "random_sleep_max": 3.0,
                    "platform_specific": {
                        "enable_get_comments": fetch_comments,
                        "enable_get_images": True,
                        "max_comments_per_note": max_comments
                    }
                }
            }
            
            logger.info(f"ğŸ”„ åˆ›å»ºé‡‡é›†ä»»åŠ¡: {task_id}, ç›®æ ‡æ•°é‡: {len(note_ids)}")
            
            response = self.session.post(
                f"{self.api_endpoint}/api/v1/tasks",
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get("task_id"):
                logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {result.get('task_id')}")
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡åˆ›å»ºå“åº”å¼‚å¸¸: {result}")
                
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        try:
            response = self.session.get(
                f"{self.api_endpoint}/api/v1/tasks/{task_id}/status",
                timeout=10
            )
            response.raise_for_status()
            
            result = response.json()
            status = result.get("status", "unknown")
            logger.info(f"ğŸ“Š ä»»åŠ¡ {task_id} çŠ¶æ€: {status}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {task_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def wait_for_task_completion(self, task_id: str, max_wait_time: int = 600, 
                                check_interval: int = 5) -> bool:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆ
        
        Args:
            task_id: ä»»åŠ¡ID
            max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        import time
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            status_result = self.get_task_status(task_id)
            
            if not status_result.get("success", True):
                # å¦‚æœè·å–çŠ¶æ€å¤±è´¥ï¼Œç­‰ä¸€ä¼šå†è¯•
                time.sleep(check_interval)
                continue
                
            status = status_result.get("status", "unknown")
            
            if status == "completed":
                logger.info(f"âœ… ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆ")
                return True
            elif status == "failed":
                logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥")
                return False
            elif status in ["running", "pending"]:
                # ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œç»§ç»­ç­‰å¾…
                progress = status_result.get("progress", {})
                percent = progress.get("progress_percent", 0)
                stage = progress.get("current_stage", "æœªçŸ¥")
                logger.info(f"â³ ä»»åŠ¡ {task_id} è¿›è¡Œä¸­: {stage} ({percent:.1f}%)")
                time.sleep(check_interval)
            else:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} çŠ¶æ€æœªçŸ¥: {status}")
                time.sleep(check_interval)
        
        logger.error(f"â° ä»»åŠ¡ {task_id} ç­‰å¾…è¶…æ—¶")
        return False

    def get_note_content_by_id(self, note_id: str) -> Dict[str, Any]:
        """
        æ ¹æ®note_idè·å–ç¬”è®°å†…å®¹
        
        Args:
            note_id: ç¬”è®°ID
            
        Returns:
            ç¬”è®°è¯¦ç»†å†…å®¹
        """
        try:
            logger.info(f"ğŸ” æŸ¥è¯¢ç¬”è®°å†…å®¹: {note_id}")
            
            response = self.session.get(
                f"{self.api_endpoint}/api/v1/data/content/xhs/{note_id}",
                timeout=10
            )
            response.raise_for_status()
            
            result = response.json()
            
            if result.get("success") and result.get("data"):
                logger.info(f"âœ… æˆåŠŸè·å–ç¬”è®°å†…å®¹: {note_id}")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç¬”è®°å†…å®¹: {note_id}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æŸ¥è¯¢ç¬”è®°å¤±è´¥: {note_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {note_id}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}

    def crawl_note(self, note_url: str, fetch_comments: bool = False) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªç¬”è®°å†…å®¹ï¼ˆé«˜å±‚æ¬¡æ¥å£ï¼Œè‡ªåŠ¨å¤„ç†ä»»åŠ¡åˆ›å»ºå’Œç­‰å¾…ï¼‰
        
        Args:
            note_url: ç¬”è®°URL
            fetch_comments: æ˜¯å¦è·å–è¯„è®º
            
        Returns:
            ç¬”è®°è¯¦ç»†å†…å®¹æ•°æ®
        """
        try:
            # ä»URLæå–note_id
            note_id = self.extract_note_id_from_url(note_url)
            if not note_id:
                raise ValueError(f"æ— æ³•ä»URLæå–note_id: {note_url}")
            
            logger.info(f"ğŸ”„ å¼€å§‹çˆ¬å–ç¬”è®°: {note_url} (ID: {note_id})")
            
            # å…ˆå°è¯•ä»æ•°æ®åº“è·å–
            existing_data = self.get_note_content_by_id(note_id)
            if existing_data.get("success") and existing_data.get("data"):
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–åˆ°ç°æœ‰æ•°æ®: {note_id}")
                return existing_data
            
            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºé‡‡é›†ä»»åŠ¡
            task_result = self.create_crawl_task(
                note_ids=[note_id],
                fetch_comments=fetch_comments
            )
            
            if not task_result.get("task_id"):
                raise Exception(f"ä»»åŠ¡åˆ›å»ºå¤±è´¥: {task_result.get('message', 'Unknown error')}")
            
            task_id = task_result["task_id"]
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            if self.wait_for_task_completion(task_id, max_wait_time=300):
                # ä»»åŠ¡å®Œæˆï¼Œè·å–ç»“æœ
                result = self.get_note_content_by_id(note_id)
                if result.get("success"):
                    logger.info(f"âœ… æˆåŠŸçˆ¬å–ç¬”è®°å†…å®¹: {note_url}")
                    return result
                else:
                    raise Exception(f"ä»»åŠ¡å®Œæˆä½†æ— æ³•è·å–æ•°æ®: {result.get('message')}")
            else:
                raise Exception(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥æˆ–è¶…æ—¶: {task_id}")
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ç¬”è®°å¤±è´¥: {note_url}, é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
    
    def batch_crawl_notes(self, note_urls: List[str], fetch_comments: bool = False) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–ç¬”è®°å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å•ä¸ªä»»åŠ¡å¤„ç†å¤šä¸ªç¬”è®°ï¼‰
        
        Args:
            note_urls: ç¬”è®°URLåˆ—è¡¨
            fetch_comments: æ˜¯å¦è·å–è¯„è®º
            
        Returns:
            ç¬”è®°è¯¦ç»†å†…å®¹åˆ—è¡¨
        """
        if not note_urls:
            return []
        
        try:
            # æå–æ‰€æœ‰note_id
            note_ids = []
            url_to_id_map = {}
            
            for url in note_urls:
                note_id = self.extract_note_id_from_url(url)
                if note_id:
                    note_ids.append(note_id)
                    url_to_id_map[note_id] = url
                else:
                    logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–note_id: {url}")
            
            if not note_ids:
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„note_id")
                return [{"success": False, "error": "æ²¡æœ‰æœ‰æ•ˆçš„note_id"}] * len(note_urls)
            
            logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡çˆ¬å– {len(note_ids)} ä¸ªç¬”è®°")
            
            # å…ˆæ£€æŸ¥å“ªäº›æ•°æ®å·²ç»å­˜åœ¨
            existing_data = {}
            new_note_ids = []
            
            for note_id in note_ids:
                existing_result = self.get_note_content_by_id(note_id)
                if existing_result.get("success") and existing_result.get("data"):
                    existing_data[note_id] = existing_result
                    logger.info(f"âœ… ä»æ•°æ®åº“è·å–åˆ°ç°æœ‰æ•°æ®: {note_id}")
                else:
                    new_note_ids.append(note_id)
            
            # å¯¹äºæ²¡æœ‰çš„æ•°æ®ï¼Œåˆ›å»ºæ‰¹é‡é‡‡é›†ä»»åŠ¡
            if new_note_ids:
                logger.info(f"ğŸš€ åˆ›å»ºæ‰¹é‡é‡‡é›†ä»»åŠ¡ï¼Œç›®æ ‡: {len(new_note_ids)} ä¸ªæ–°ç¬”è®°")
                
                task_result = self.create_crawl_task(
                    note_ids=new_note_ids,
                    fetch_comments=fetch_comments
                )
                
                if task_result.get("task_id"):
                    task_id = task_result["task_id"]
                    
                    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                    if self.wait_for_task_completion(task_id, max_wait_time=600):
                        logger.info(f"âœ… æ‰¹é‡é‡‡é›†ä»»åŠ¡å®Œæˆ: {task_id}")
                        
                        # è·å–æ–°é‡‡é›†çš„æ•°æ®
                        for note_id in new_note_ids:
                            result = self.get_note_content_by_id(note_id)
                            if result.get("success"):
                                existing_data[note_id] = result
                            else:
                                existing_data[note_id] = {"success": False, "error": f"æ— æ³•è·å–æ•°æ®: {note_id}"}
                    else:
                        logger.error(f"âŒ æ‰¹é‡é‡‡é›†ä»»åŠ¡å¤±è´¥æˆ–è¶…æ—¶: {task_id}")
                        # ä¸ºå¤±è´¥çš„note_idåˆ›å»ºé”™è¯¯ç»“æœ
                        for note_id in new_note_ids:
                            existing_data[note_id] = {"success": False, "error": f"é‡‡é›†ä»»åŠ¡å¤±è´¥: {task_id}"}
                else:
                    logger.error(f"âŒ åˆ›å»ºæ‰¹é‡ä»»åŠ¡å¤±è´¥: {task_result.get('message')}")
                    # ä¸ºæ‰€æœ‰æ–°note_idåˆ›å»ºé”™è¯¯ç»“æœ
                    for note_id in new_note_ids:
                        existing_data[note_id] = {"success": False, "error": "åˆ›å»ºé‡‡é›†ä»»åŠ¡å¤±è´¥"}
            
            # æŒ‰åŸå§‹URLé¡ºåºç»„ç»‡ç»“æœ
            results = []
            for url in note_urls:
                note_id = self.extract_note_id_from_url(url)
                if note_id and note_id in existing_data:
                    results.append(existing_data[note_id])
                else:
                    results.append({"success": False, "error": f"æ— æ³•å¤„ç†URL: {url}"})
            
            success_count = len([r for r in results if r.get("success", False)])
            logger.info(f"ğŸ¯ æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}/{len(note_urls)}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
            return [{"success": False, "error": str(e)}] * len(note_urls)
    
    def health_check(self) -> bool:
        """æ£€æŸ¥APIæœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.api_endpoint}/health", timeout=5)
            return response.status_code == 200
        except:
            return False


def create_mediacrawler_client() -> MediaCrawlerClient:
    """åˆ›å»ºMediaCrawlerå®¢æˆ·ç«¯å®ä¾‹"""
    return MediaCrawlerClient()


# ä¾¿æ·å‡½æ•°
def fetch_note_content(note_url: str, fetch_comments: bool = False) -> Dict[str, Any]:
    """è·å–å•ä¸ªç¬”è®°å†…å®¹çš„ä¾¿æ·å‡½æ•°"""
    client = create_mediacrawler_client()
    return client.crawl_note(note_url, fetch_comments)


def batch_fetch_note_contents(note_urls: List[str], fetch_comments: bool = False) -> List[Dict[str, Any]]:
    """æ‰¹é‡è·å–ç¬”è®°å†…å®¹çš„ä¾¿æ·å‡½æ•°"""
    client = create_mediacrawler_client()
    return client.batch_crawl_notes(note_urls, fetch_comments)


def test_url_extraction():
    """æµ‹è¯•URLè§£æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•URLè§£æåŠŸèƒ½...")
    client = create_mediacrawler_client()
    
    test_cases = [
        # æµ‹è¯•ç”¨ä¾‹: (è¾“å…¥URL, æœŸæœ›çš„note_id)
        ("68622a8b0000000015020c92", "68622a8b0000000015020c92"),  # çº¯note_id
        ("https://www.xiaohongshu.com/explore/68622a8b0000000015020c92?xsec_token=ZBuyva3FFEKzv_CBjmw6dEClLM879Okee6liwo_ZdFl4M=&xsec_source=pc_ad", "68622a8b0000000015020c92"),  # æ–°æ ¼å¼
        ("https://www.xiaohongshu.com/note/676a4d0a000000001f00c58a", "676a4d0a000000001f00c58a"),  # æ—§æ ¼å¼
        ("https://xiaohongshu.com/note/676a4d0a000000001f00c58a", "676a4d0a000000001f00c58a"),  # æ— www
        ("https://www.xiaohongshu.com/explore/676a4d0a000000001f00c58a", "676a4d0a000000001f00c58a"),  # exploreæ ¼å¼
        ("invalid_url", None),  # æ— æ•ˆURL
    ]
    
    for test_url, expected in test_cases:
        result = client.extract_note_id_from_url(test_url)
        status = "âœ…" if result == expected else "âŒ"
        print(f"{status} {test_url[:50]}... -> {result} (æœŸæœ›: {expected})")
    
    print()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import logging
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•URLè§£æ
    test_url_extraction()
    
    # æµ‹è¯•APIè¿æ¥
    client = create_mediacrawler_client()
    
    print("ğŸ§ª æµ‹è¯•MediaCrawler APIè¿æ¥...")
    if client.health_check():
        print("âœ… APIæœåŠ¡å™¨è¿æ¥æ­£å¸¸")
        
        # å¦‚æœAPIå¯ç”¨ï¼Œæµ‹è¯•ç¬”è®°çˆ¬å–
        test_url = "https://www.xiaohongshu.com/explore/68622a8b0000000015020c92"
        print(f"\nğŸ§ª æµ‹è¯•ç¬”è®°çˆ¬å–: {test_url}")
        result = client.crawl_note(test_url)
        print(f"ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)[:500]}...")
    else:
        print("âŒ APIæœåŠ¡å™¨è¿æ¥å¤±è´¥ï¼Œè·³è¿‡çˆ¬å–æµ‹è¯•") 