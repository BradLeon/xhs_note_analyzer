#!/usr/bin/env python
"""
æµ‹è¯•è„šæœ¬ - Step2: é‡‡é›†ç¬”è®°è¯¦ç»†å†…å®¹
åŸºäºStep1çš„ç»“æœè¿›è¡Œæ•°æ®é‡‡é›†
"""

import asyncio
import json
import logging
import sys
from pathlib import Path
from typing import List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from xhs_note_analyzer.tools.mediacrawler_client import MediaCrawlerClient
from xhs_note_analyzer.main import NoteData, NoteContentData

# é…ç½®æµ‹è¯•æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_step2_fetch_note_content.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_step1_results(step1_output_dir: str = "tests/output/step1") -> List[NoteData]:
    """ä»Step1çš„ç»“æœä¸­åŠ è½½ç¬”è®°æ•°æ®"""
    try:
        # å°è¯•ä»test_result.jsonåŠ è½½
        test_result_file = Path(step1_output_dir) / "test_result.json"
        if test_result_file.exists():
            with open(test_result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if "result" in data and "data" in data["result"]:
                    note_list = data["result"]["data"]["note_data_list"]
                    return [NoteData(**note) for note in note_list]
        
        # å°è¯•å¯»æ‰¾å…¶ä»–å¯èƒ½çš„æ–‡ä»¶
        for json_file in Path(step1_output_dir).glob("*.json"):
            if json_file.name != "test_result.json":
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if "notes" in data:
                            return [NoteData(**note) for note in data["notes"]]
                except Exception as e:
                    logger.warning(f"æ— æ³•ä»{json_file}åŠ è½½æ•°æ®: {e}")
        
        logger.warning("æœªæ‰¾åˆ°Step1çš„ç»“æœæ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return create_mock_note_data()
        
    except Exception as e:
        logger.error(f"åŠ è½½Step1ç»“æœå¤±è´¥: {e}")
        return create_mock_note_data()

def create_mock_note_data() -> List[NoteData]:
    """åˆ›å»ºæ¨¡æ‹Ÿçš„ç¬”è®°æ•°æ®ç”¨äºæµ‹è¯•ï¼Œä½¿ç”¨çœŸå®çš„å¸¦tokenå®Œæ•´URL"""
    # ä½¿ç”¨å‚è€ƒä»£ç ä¸­çš„çœŸå®æµ‹è¯•URL
    return [
        NoteData(
            note_id="65728c2a000000003403fc88",
            note_title="å›½ä¼æ±‚èŒæ”»ç•¥åˆ†äº«",
            note_url="https://www.xiaohongshu.com/explore/65728c2a000000003403fc88?xsec_token=ZBBMPkZKZC66wNYHvcBT26aYWhVGGRQZBgbpYzClweEPc=&xsec_source=pc_ad",
            impression=50000, click=8000, like=1200, collect=800, comment=150, engage=2150
        ),
        NoteData(
            note_id="67d95153000000001d01d6a5",
            note_title="å¤®ä¼é¢è¯•æŠ€å·§å¤§å…¨",
            note_url="https://www.xiaohongshu.com/explore/67d95153000000001d01d6a5?xsec_token=ZBfzpWi9xD-KsHutTewgjNpM-hqNu6ymhBK86y05hmiVk=&xsec_source=pc_ad", 
            impression=30000, click=5000, like=800, collect=500, comment=100, engage=1400
        ),
        NoteData(
            note_id="676a4d0a000000001f00c58c",
            note_title="æ±‚èŒç®€å†æ¨¡æ¿åˆ†äº«",
            note_url="https://www.xiaohongshu.com/explore/676a4d0a000000001f00c58c?xsec_token=MNOpqr456_mockToken3_STUVWX&xsec_source=pc_ad",
            impression=40000, click=6500, like=1000, collect=600, comment=120, engage=1720
        )
    ]

def test_mediacrawler_connection():
    """æµ‹è¯•MediaCrawler APIè¿æ¥"""
    print("ğŸ”§ æµ‹è¯• MediaCrawler API è¿æ¥...")
    
    try:
        # åˆ›å»ºå¸¦è°ƒè¯•åŠŸèƒ½çš„å®¢æˆ·ç«¯
        client = MediaCrawlerClient(debug_requests=True)
        
        # æ£€æŸ¥å¥åº·çŠ¶æ€
        print("\nğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥...")
        is_healthy = client.health_check()
        print(f"APIå¥åº·çŠ¶æ€: {'âœ… æ­£å¸¸' if is_healthy else 'âŒ ä¸å¯ç”¨'}")
        
        if not is_healthy:
            print("âš ï¸ MediaCrawler APIä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        
        return is_healthy, client
        
    except Exception as e:
        logger.warning(f"MediaCrawlerè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        print(f"âŒ APIè¿æ¥å¼‚å¸¸: {e}")
        return False, None

def test_note_id_extraction():
    """æµ‹è¯•note_idæå–åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯• note_id æå–åŠŸèƒ½...")
    
    client = MediaCrawlerClient(debug_requests=False)  # IDæå–ä¸éœ€è¦è°ƒè¯•HTTPè¯·æ±‚
    
    test_cases = [
        ("https://www.xiaohongshu.com/explore/65728c2a000000003403fc88", "65728c2a000000003403fc88"),
        ("https://www.xiaohongshu.com/note/65728c2a000000003403fc88", "65728c2a000000003403fc88"),
        ("https://www.xiaohongshu.com/explore/65728c2a000000003403fc88?xsec_token=ZBBMPkZKZC66wNYHvcBT26aYWhVGGRQZBgbpYzClweEPc=&xsec_source=pc_ad", "65728c2a000000003403fc88"),  # å¸¦tokençš„URL
        ("65728c2a000000003403fc88", "65728c2a000000003403fc88"),  # çº¯ID
    ]
    
    success_count = 0
    for test_url, expected in test_cases:
        result = client.extract_note_id_from_url(test_url)
        status = "âœ…" if result == expected else "âŒ"
        print(f"  {status} {test_url[:40]}... -> {result}")
        if result == expected:
            success_count += 1
    
    print(f"note_idæå–æµ‹è¯•: {success_count}/{len(test_cases)} é€šè¿‡")
    return success_count == len(test_cases)

async def test_step2_fetch_content_async():
    """æµ‹è¯•Step2ï¼šé‡‡é›†ç¬”è®°è¯¦ç»†å†…å®¹"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯• Step2: é‡‡é›†ç¬”è®°è¯¦ç»†å†…å®¹")
    print("=" * 60)
    
    try:
        # åŠ è½½Step1çš„ç»“æœ
        print("ğŸ“¥ åŠ è½½ Step1 ç»“æœæ•°æ®...")
        note_list = load_step1_results()
        note_list = note_list[:5]
        print(f"åŠ è½½åˆ° {len(note_list)} æ¡ç¬”è®°æ•°æ®")
        
        if not note_list:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„ç¬”è®°æ•°æ®")
            return False
        
        # æ˜¾ç¤ºå°†è¦å¤„ç†çš„ç¬”è®°
        print("\nğŸ“‹ å¾…å¤„ç†çš„ç¬”è®°:")
        for i, note in enumerate(note_list, 1):  # åªå¤„ç†å‰5æ¡
            print(f"  {i}. {note.note_title}")
            print(f"     URL: {note.note_url}")
        
        # æ£€æŸ¥APIè¿æ¥
        api_available, client = test_mediacrawler_connection()
        
        output_dir = "tests/output/step2"
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # å¼€å§‹é‡‡é›†å†…å®¹ - ä½¿ç”¨æ‰¹é‡æ–¹å¼
        print(f"\nğŸ” å¼€å§‹æ‰¹é‡é‡‡é›†è¯¦ç»†å†…å®¹...")
        detailed_notes = []
        note_urls = [note.note_url for note in note_list]  # é™åˆ¶åªå¤„ç†å‰5æ¡
        
        if api_available and client:
            try:
                # ä½¿ç”¨æ–°çš„APIæµç¨‹: åˆ›å»ºä»»åŠ¡ -> ç›‘æ§ä»»åŠ¡ -> æŸ¥è¯¢ç»“æœ
                print(f"  ğŸ“¡ ä½¿ç”¨ MediaCrawler APIé‡‡é›† {len(note_urls)} ä¸ªç¬”è®°...")
                print(f"  ğŸ“‹ æµ‹è¯•URLæ ¼å¼: {[url[:80] + '...' for url in note_urls]}")
                
                # ä½¿ç”¨å®Œæ•´çš„APIæµç¨‹
                results = client.batch_crawl_notes(note_urls, fetch_comments=False)
                
                # å¤„ç†æ‰¹é‡é‡‡é›†ç»“æœ
                for i, (note, result) in enumerate(zip(note_list, results), 1):
                    print(f"\nå¤„ç†ç¬”è®° {i}/{len(note_list)}: {note.note_title}")
                    
                    if result.get("success") and result.get("data"):
                        # APIæˆåŠŸï¼Œè½¬æ¢æ•°æ®
                        detailed_content = convert_api_result_to_note_content(note, result)
                        print("  âœ… APIé‡‡é›†æˆåŠŸ")
                    
                    detailed_notes.append(detailed_content)
                
                print(f"  âœ… æ‰¹é‡é‡‡é›†å®Œæˆ")
                
            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
                print(f"  âš ï¸ APIå¼‚å¸¸ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
         
        # ä¿å­˜ç»“æœ
        print(f"\nğŸ’¾ ä¿å­˜é‡‡é›†ç»“æœ...")
        
        # ä¿å­˜ä¸ºJSON
        result_file = Path(output_dir) / "step2_content_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": str(asyncio.get_event_loop().time()),
                "total_notes": len(detailed_notes),
                "detailed_notes": [note.model_dump() for note in detailed_notes]
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦
        summary_file = Path(output_dir) / "step2_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("Step2 ç¬”è®°å†…å®¹é‡‡é›†ç»“æœæ‘˜è¦\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"å¤„ç†ç¬”è®°æ•°: {len(detailed_notes)}\n")
            f.write(f"APIçŠ¶æ€: {'å¯ç”¨' if api_available else 'ä¸å¯ç”¨'}\n\n")
            
            f.write("è¯¦ç»†å†…å®¹:\n")
            f.write("-" * 30 + "\n")
            for i, note in enumerate(detailed_notes, 1):
                f.write(f"\n{i}. {note.title}\n")
                f.write(f"   ID: {note.note_id}\n")
                f.write(f"   å†…å®¹é•¿åº¦: {len(note.content)} å­—ç¬¦\n")
                f.write(f"   å›¾ç‰‡æ•°é‡: {len(note.images)}\n")
                f.write(f"   æ ‡ç­¾æ•°é‡: {len(note.tags)}\n")
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  ğŸ“„ è¯¦ç»†æ•°æ®: {result_file}")
        print(f"  ğŸ“‹ æ‘˜è¦ä¿¡æ¯: {summary_file}")
        
        # éªŒè¯ç»“æœ
        print(f"\nğŸ” éªŒè¯é‡‡é›†ç»“æœ:")
        success_count = 0
        api_success_count = 0
        mock_count = 0
        
        for i, note in enumerate(detailed_notes):

            
            # æ£€æŸ¥åŸºæœ¬æ•°æ®è´¨é‡
            has_meaningful_content = len(note.content) > 50  # è‡³å°‘æœ‰50ä¸ªå­—ç¬¦
            has_basic_info = note.title and len(note.title) > 1
  
            if has_meaningful_content and has_basic_info:
                # APIæˆåŠŸé‡‡é›†çš„æ•°æ®ï¼Œæ— è®ºå›¾ç‰‡æ•°é‡å¤šå°‘éƒ½ç®—æˆåŠŸ
                status = "âœ…"  # APIæˆåŠŸ
                data_source = "APIé‡‡é›†"
                api_success_count += 1
                success_count += 1
            else:
                status = "âŒ"  # å¤±è´¥
                data_source = "æ•°æ®ä¸å®Œæ•´"
            
            print(f"  {status} {note.title}: å†…å®¹({len(note.content)}å­—ç¬¦) å›¾ç‰‡({len(note.images)}å¼ ) [{data_source}]")
            
          
        print(f"\nğŸ“Š Step2 æµ‹è¯•ç»“æœ:")
        print(f"APIæˆåŠŸé‡‡é›†: {api_success_count}/{len(detailed_notes)} ({api_success_count/len(detailed_notes)*100:.1f}%)")
        print(f"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {mock_count}/{len(detailed_notes)} ({mock_count/len(detailed_notes)*100:.1f}%)")
        print(f"æ€»ä½“æˆåŠŸç‡: {(api_success_count + mock_count)/len(detailed_notes)*100:.1f}%")
        
        # è¿”å›æˆåŠŸæ¡ä»¶ï¼šè‡³å°‘æœ‰60%çš„ç¬”è®°é€šè¿‡APIæˆåŠŸé‡‡é›†ï¼Œæˆ–æ€»ä½“æˆåŠŸç‡ï¼ˆAPI+æ¨¡æ‹Ÿï¼‰è¾¾åˆ°80%
        total_valid = api_success_count + mock_count
        success_rate = total_valid / len(detailed_notes)
        api_success_rate = api_success_count / len(detailed_notes)
        
        # å¦‚æœAPIæˆåŠŸç‡è¶…è¿‡60%ï¼Œæˆ–è€…æ€»ä½“æˆåŠŸç‡è¶…è¿‡80%ï¼Œéƒ½è®¤ä¸ºæµ‹è¯•é€šè¿‡
        return api_success_rate >= 0.6 or success_rate >= 0.8
        
        
    except Exception as e:
        logger.error(f"âŒ Step2 æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def convert_api_result_to_note_content(note: NoteData, api_result: dict) -> NoteContentData:
    """å°†APIè¿”å›ç»“æœè½¬æ¢ä¸ºNoteContentDataæ ¼å¼ï¼Œé€‚é…æ–°çš„APIå“åº”æ ¼å¼"""
    raw_data = api_result.get("data", {})
    
    # å¤„ç†å›¾ç‰‡URLåˆ—è¡¨
    images = []
    if raw_data.get("images"):
        images = raw_data["images"] if isinstance(raw_data["images"], list) else [raw_data["images"]]
    elif raw_data.get("image_list"):
        images = raw_data["image_list"] if isinstance(raw_data["image_list"], list) else [raw_data["image_list"]]
    
    # å¤„ç†ä½œè€…ä¿¡æ¯ - é€‚é…å¤šç§å­—æ®µå
    author_info = {}
    if raw_data.get("nickname"):
        author_info["name"] = raw_data["nickname"] 
    elif raw_data.get("user_name"):
        author_info["name"] = raw_data["user_name"]
    
    if raw_data.get("user_id"):
        author_info["user_id"] = raw_data["user_id"]
    if raw_data.get("follower_count"):
        author_info["followers"] = raw_data["follower_count"]
    
    # å¤„ç†æ ‡ç­¾
    tags = []
    if raw_data.get("tags"):
        tags = raw_data["tags"] if isinstance(raw_data["tags"], list) else [raw_data["tags"]]
    elif raw_data.get("note_tag_list"):
        # ä»note_tag_listæå–æ ‡ç­¾å
        tag_list = raw_data["note_tag_list"]
        if isinstance(tag_list, list):
            tags = [tag.get("name", str(tag)) for tag in tag_list if tag]
    
    # å¤„ç†è§†é¢‘URL
    video_url = ""
    if raw_data.get("video_url"):
        video_url = raw_data["video_url"]
    elif raw_data.get("video") and isinstance(raw_data["video"], dict):
        video_url = raw_data["video"].get("url", "")
    
    # å¤„ç†å‘å¸ƒæ—¶é—´
    create_time = ""
    if raw_data.get("last_update_time"):
        create_time = raw_data["last_update_time"]
    elif raw_data.get("publish_time"):
        create_time = raw_data["publish_time"]
    elif raw_data.get("create_time"):
        create_time = raw_data["create_time"]
    
    return  NoteContentData(
        note_id=raw_data.get("note_id"),
        title=raw_data.get("title"),
        basic_info=note,
        content=raw_data.get("desc", raw_data.get("content", f"è¿™æ˜¯{note.note_title}çš„è¯¦ç»†å†…å®¹...")),
        images=images,
        video_url=video_url,
        author_info=author_info,
        tags=tags,
        create_time=create_time
    )


# Removed unused async functions - using batch_crawl_notes instead


def validate_step2_output(output_dir: str):
    """éªŒè¯Step2çš„è¾“å‡ºæ–‡ä»¶"""
    print("\nğŸ” éªŒè¯ Step2 è¾“å‡ºæ–‡ä»¶...")
    
    output_path = Path(output_dir)
    expected_files = [
        "step2_content_results.json",
        "step2_summary.txt"
    ]
    
    missing_files = []
    for file_name in expected_files:
        file_path = output_path / file_name
        if not file_path.exists():
            missing_files.append(file_name)
        else:
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {file_path}")
    
    if missing_files:
        print(f"âš ï¸ ç¼ºå¤±æ–‡ä»¶: {missing_files}")
    else:
        print("âœ… æ‰€æœ‰é¢„æœŸæ–‡ä»¶éƒ½å·²ç”Ÿæˆ")
    
    return len(missing_files) == 0

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Step2 é‡‡é›†ç¬”è®°è¯¦ç»†å†…å®¹ - æµ‹è¯•è„šæœ¬")
    print("=" * 80)
    
    # æµ‹è¯•å‰ç½®æ£€æŸ¥
    print("ğŸ”§ å‰ç½®æ£€æŸ¥...")
    id_extraction_ok = test_note_id_extraction()
    
    # æ‰§è¡Œä¸»è¦æµ‹è¯•  
    success1 = await test_step2_fetch_content_async()
    
    # éªŒè¯è¾“å‡º
    success2 = validate_step2_output("tests/output/step2")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š Step2 æµ‹è¯•æ€»ç»“:")
    print(f"IDæå–åŠŸèƒ½: {'âœ… é€šè¿‡' if id_extraction_ok else 'âŒ å¤±è´¥'}")
    print(f"å†…å®¹é‡‡é›†æµ‹è¯•: {'âœ… é€šè¿‡' if success1 else 'âŒ å¤±è´¥'}")
    print(f"è¾“å‡ºéªŒè¯æµ‹è¯•: {'âœ… é€šè¿‡' if success2 else 'âŒ å¤±è´¥'}")
    
    overall_success = id_extraction_ok and success1 and success2
    print(f"\nğŸ¯ æ•´ä½“æµ‹è¯•ç»“æœ: {'âœ… å…¨éƒ¨é€šè¿‡' if overall_success else 'âŒ å­˜åœ¨å¤±è´¥'}")
    
    if overall_success:
        print("\nâœ¨ Step2 æµ‹è¯•å®Œæˆï¼Œå¯ä»¥ç»§ç»­æ‰§è¡Œ Step3 æµ‹è¯•")
        print("ğŸ’¡ æç¤º: Step2 çš„ç»“æœå°†è¢« Step3 ä½¿ç”¨è¿›è¡Œå†…å®¹åˆ†æ")
    else:
        print("\nâš ï¸ è¯·æ£€æŸ¥å¹¶ä¿®å¤å¤±è´¥çš„æµ‹è¯•é¡¹")
    
    return overall_success

if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    import os
    api_endpoint = os.getenv("MEDIACRAWLER_API_ENDPOINT", "http://localhost:8000")
    print(f"ğŸ”§ MediaCrawler APIåœ°å€: {api_endpoint}")
    
    if not os.getenv("OPENROUTER_API_KEY"):
        print("âš ï¸ è­¦å‘Š: æœªè®¾ç½® OPENROUTER_API_KEY ç¯å¢ƒå˜é‡")
    
    print()
    
    # è¿è¡Œæµ‹è¯•
    result = asyncio.run(main())
    
    # é€€å‡ºç 
    sys.exit(0 if result else 1)